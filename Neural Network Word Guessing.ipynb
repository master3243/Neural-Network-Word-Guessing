{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(mini_batch_size):\n",
    "    mat = scipy.io.loadmat('data.mat')\n",
    "    data = mat['data'][0][0]\n",
    "    words_per_input = data[0].shape[0] - 1\n",
    "    \n",
    "    test_input = data[0][0:words_per_input].T\n",
    "    test_input -= 1\n",
    "    test_target = data[0][words_per_input].T\n",
    "    test_target -= 1\n",
    "    \n",
    "    train_input = data[1][0:words_per_input].T\n",
    "    # minus one because matlab starts counting from 1 not 0\n",
    "    train_input -= 1\n",
    "    train_target = data[1][words_per_input].T\n",
    "    train_target -= 1\n",
    "\n",
    "    ex = train_input.shape[0]//mini_batch_size * mini_batch_size\n",
    "    # discard last couple of examples to make number of examples divisible by mini_batch_size\n",
    "    train_input = train_input[0:ex]\n",
    "    train_target = train_target[0:ex]\n",
    "    train_input = train_input.reshape(-1, mini_batch_size, words_per_input)\n",
    "    train_target = train_target.reshape(-1, mini_batch_size)\n",
    "\n",
    "    valid_input = data[2][0:words_per_input].T\n",
    "    valid_input -= 1\n",
    "    valid_target = data[2][words_per_input].T\n",
    "    valid_target -= 1\n",
    "    \n",
    "    result_data = (train_input, train_target, valid_input, valid_target, test_input, test_target)\n",
    "    vocab = np.array([n[0] for n in data[3][0]])\n",
    "    return (result_data, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL VARIABLES\n",
    "\n",
    "# HYPERPARAMETERS:\n",
    "mini_batch_size = 100\n",
    "learning_rate = 0.1;  # Learning rate; default = 0.1.\n",
    "momentum = 0.9  # Momentum; default = 0.9.\n",
    "numhid1 = 50;   # Dimensionality of embedding space; default = 50.\n",
    "numhid2 = 200;   # Number of units in hidden layer; default = 200.\n",
    "init_wt = 0.01;  # Standard deviation of the normal distribution which is sampled to get the initial weights; default = 0.01\n",
    "\n",
    "# VARIABLES FOR TRACKING TRAINING PROGRESS.\n",
    "show_training_CE_after = 300;\n",
    "show_validation_CE_after = 1000;\n",
    "\n",
    "# constants\n",
    "data, vocab = load_data(mini_batch_size)\n",
    "words_per_input = data[0].shape[2] # shape[0]->num_mini_batches shape[1]->ex_per_mini_batch shape[2]->words_per_example\n",
    "vocab_size = vocab.shape[0]\n",
    "one_hot_expantion = np.identity(vocab_size)\n",
    "\n",
    "a = train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c42baa1a6e6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#         w += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# print(c, w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def get_word(n):\n",
    "    return vocab[n]\n",
    "\n",
    "def get_words(num_list):\n",
    "    return [get_word(n) for n in num_list]\n",
    "\n",
    "def predict(model, input_batch):\n",
    "    output = fprop(input_batch, *model)[2]\n",
    "    return get_words(np.argmax(output, axis=1))\n",
    "\n",
    "\n",
    "# pred = predict(model, data[4][:])\n",
    "# target = get_word(data[5][:])\n",
    "# c = 0\n",
    "# w = 0\n",
    "# for n in (zip(pred, target)):\n",
    "#     if n[0] == n[1]:\n",
    "#         print(n[0])\n",
    "#         c += 1\n",
    "#     else:\n",
    "#         w += 1\n",
    "# print(c, w)\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n",
      "> <ipython-input-21-f90c673166af>(108)bprop()\n",
      "-> return [embedding_gradient, hidden_gradient, hidden_bias_gradient, output_gradient, output_bias_gradient]\n",
      "(Pdb) output_z_gradient.shape\n",
      "(100, 250)\n",
      "(Pdb) output_layer_state.shape\n",
      "(100, 250)\n",
      "(Pdb) output_gradient.shape\n",
      "(200, 250)\n",
      "(Pdb) output_bias_gradient.shape\n",
      "(250,)\n"
     ]
    }
   ],
   "source": [
    "def init_training_parameters():\n",
    "    # input: 3 words -> 3 250-one-hot vectors\n",
    "    # embedding: each 250-one-hot-vector goes through the same encoding proccess (table look up) using word_embedding_weights, no sigmoid. \n",
    "    # hidden: 3*numhid1 neurons go to one big hidden layer. 3*numhid1 -> numhid2. sigmoid activation\n",
    "    # output: numhid2 -> 250-one-hot-vector softmax\n",
    "    weights = [0, 0, 0, 0, 0]\n",
    "    deltas = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    # each input to word embedding layer\n",
    "    shape = (vocab_size, numhid1)\n",
    "    weights[0] = np.random.normal(scale=init_wt, size=shape)\n",
    "    deltas[0] = np.zeros(shape)\n",
    "    \n",
    "    # all word embeddings to single hidden layer\n",
    "    shape = (words_per_input*numhid1, numhid2)\n",
    "    weights[1] = np.random.normal(scale=init_wt, size=shape)\n",
    "    deltas[1] = np.zeros(shape)\n",
    "    \n",
    "    # hidden bias\n",
    "    shape = (numhid2)\n",
    "    weights[2] = np.zeros(shape)\n",
    "    deltas[2] = np.zeros(shape)\n",
    "    \n",
    "    # hidden to output layer\n",
    "    shape = (numhid2, vocab_size)\n",
    "    weights[3] = np.random.normal(scale=init_wt, size=shape)\n",
    "    deltas[3] = np.zeros(shape)\n",
    "    \n",
    "    # output bias\n",
    "    shape = (vocab_size)\n",
    "    weights[4] = np.zeros(shape)\n",
    "    deltas[4] = np.zeros(shape)\n",
    "    \n",
    "    return (weights, deltas)\n",
    "\n",
    "def fprop(input_batch, embedding_weights, hidden_weights, hidden_bias, output_weights, output_bias):\n",
    "    \"\"\"forward prop:\n",
    "        output:\n",
    "        embedding_layer_states: np.array with shape (num_examples, num_words_per_example * numhid1)\n",
    "        hidden_layer_state: np.array with shape (num_examples, numhid2)\n",
    "        output_layer_state: np.array with shape (num_examples, vocab_size)\"\"\"\n",
    "    if input_batch.ndim == 1:\n",
    "        # input is 1d single training example, must promote to 2d single row array to not cause errors\n",
    "        input_batch = input_batch[None, :]\n",
    "    ### embedding_layer_states ###\n",
    "    # input_batch.shape == (100, 3), np.min(input_batch) == 0, np.max(input_batch) == 249\n",
    "    # word_embedding_weights.shape == (250, 50)\n",
    "    # replace each element in input_batch with word_embedding_weights[element][:] --> table look up\n",
    "    embedding_layer_state = embedding_weights[input_batch]\n",
    "    # embedding_layer_states.shape == (100, 3, 50)\n",
    "    # combine 3 sets of 50 neurons into one set of 150 neurons\n",
    "    embedding_layer_state = np.reshape(embedding_layer_state, (input_batch.shape[0], -1))\n",
    "    # embedding_layer_states.shape == (100, 150)\n",
    "    \n",
    "    ### hidden_layer_state ###\n",
    "    hidden_layer_input = np.dot(embedding_layer_state, hidden_weights) + hidden_bias\n",
    "    # apply sigmoid to input to get states\n",
    "    hidden_layer_state = 1 / (1 + np.exp(-hidden_layer_input))\n",
    "    \n",
    "    ### output_layer_states ###\n",
    "    ## SHAPES:\n",
    "    # output_layer_input (100, 250) \n",
    "    # np.max(output_layer_input) single number \n",
    "    # numerator (100, 250) \n",
    "    # np.sum(numerator, axis=1) (100,) \n",
    "    # denominator (100, 1)\n",
    "    output_layer_input = np.dot(hidden_layer_state, output_weights) + output_bias\n",
    "    # apply softmax to input to get states\n",
    "    numerator = np.exp(output_layer_input - np.max(output_layer_input))\n",
    "    denominator = np.reshape(np.sum(numerator, axis=1), (-1, 1))\n",
    "    output_layer_state = numerator / denominator\n",
    "\n",
    "    return (embedding_layer_state, hidden_layer_state, output_layer_state)\n",
    "\n",
    "def loss_function(output_layer_state, target):\n",
    "    num_examples = target.shape[0]\n",
    "    return -np.sum(target * np.log(output_layer_state+1e-12))/num_examples\n",
    "\n",
    "def accuracy(output_layer_state, target):\n",
    "    pred = np.argmax(output_layer_state, axis=1)\n",
    "    target = np.argmax(target, axis=1)\n",
    "    return np.mean(pred == target)\n",
    "\n",
    "def bprop(input_batch, target, \n",
    "          embedding_layer_state, hidden_layer_state, output_layer_state, \n",
    "          embedding_weights, hidden_weights, hidden_bias, output_weights, output_bias):\n",
    "    # output layer and weights coming into it\n",
    "    output_z_gradient = output_layer_state - target\n",
    "    output_gradient = np.dot(hidden_layer_state.T, output_z_gradient)\n",
    "    output_bias_gradient = np.sum(output_z_gradient, axis=0)\n",
    "\n",
    "    # hidden layer and weights coming into it\n",
    "    hidden_y_gradient = np.dot(output_z_gradient, output_weights.T)\n",
    "    hidden_z_gradient = hidden_layer_state * (1 - hidden_layer_state) * hidden_y_gradient\n",
    "    hidden_gradient = np.dot(embedding_layer_state.T, hidden_z_gradient)\n",
    "    hidden_bias_gradient = np.sum(hidden_z_gradient, axis=0)\n",
    "    \n",
    "    # embedding layer and weights coming into it\n",
    "    embedding_y_gradient = np.dot(hidden_z_gradient, hidden_weights.T)\n",
    "    # embedding_z_gradient = embedding_y_gradient because no activation function\n",
    "    embedding_gradient = 0\n",
    "    input_batch = one_hot_expantion[input_batch.T] # input_batch.shape == (3, 100, 250)\n",
    "    splits = embedding_layer_state.shape[1]//embedding_weights.shape[1]\n",
    "    step = embedding_weights.shape[1]\n",
    "    for n in range(splits):\n",
    "        embedding_gradient += np.dot(input_batch[n].T, embedding_y_gradient[:, n*step : (n+1)*step])\n",
    "    import pdb; pdb.set_trace()\n",
    "    return [embedding_gradient, hidden_gradient, hidden_bias_gradient, output_gradient, output_bias_gradient]\n",
    "\n",
    "def train(epochs):\n",
    "    start_time = time.time()\n",
    "    # init parameters\n",
    "    all_weights, all_deltas = init_training_parameters()\n",
    "    # get data\n",
    "    train_input, train_target, valid_input, valid_target, test_input, test_target = data\n",
    "    valid_target = one_hot_expantion[valid_target]\n",
    "    test_target = one_hot_expantion[test_target]\n",
    "    \n",
    "    batches_per_chunk = show_training_CE_after\n",
    "    batches_every_validation = show_validation_CE_after\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('epoch: ', epoch + 1)\n",
    "        chunk_trainset_CE = 0\n",
    "        chunk_batch_count = 0\n",
    "        epoch_train_CE = 0\n",
    "        \n",
    "        for mini_batch_num in range(train_input.shape[0]):\n",
    "            mini_batch = train_input[mini_batch_num]\n",
    "            mini_batch_target = train_target[mini_batch_num]\n",
    "            mini_batch_target = one_hot_expantion[mini_batch_target]\n",
    "            \n",
    "            # forward propagation\n",
    "            layer_states = fprop(mini_batch, *all_weights)\n",
    "            \n",
    "            # calculate cross entropy\n",
    "            CE = loss_function(layer_states[2], mini_batch_target)\n",
    "            epoch_train_CE = (CE + mini_batch_num*epoch_train_CE) / (mini_batch_num + 1)\n",
    "            \n",
    "            # print training cross entropy\n",
    "            chunk_trainset_CE = (CE + chunk_batch_count*chunk_trainset_CE) / (chunk_batch_count + 1)\n",
    "            chunk_batch_count += 1\n",
    "            if (mini_batch_num+1) % batches_per_chunk == 0:\n",
    "                print('Batch {0}: train CE {1}'.format(mini_batch_num+1, chunk_trainset_CE))\n",
    "                print('Accuracy {0}'.format(accuracy(layer_states[2], mini_batch_target)))\n",
    "                chunk_batch_count = 0\n",
    "                chunk_trainset_CE = 0\n",
    "            # backward propagation\n",
    "            all_gradients = bprop(mini_batch, mini_batch_target, *layer_states, *all_weights)\n",
    "            for n in range(len(all_gradients)):\n",
    "                all_deltas[n] = all_deltas[n] * momentum + all_gradients[n] / mini_batch_size\n",
    "                all_weights[n] -= learning_rate * all_deltas[n]\n",
    "            # print validation\n",
    "            if (mini_batch_num+1) % batches_every_validation == 0:\n",
    "                layer_states = fprop(valid_input, *all_weights)\n",
    "                validation_CE = loss_function(layer_states[2], valid_target)\n",
    "                print('Batch {0}: validation CE {1}'.format(mini_batch_num+1, validation_CE))\n",
    "                print('Accuracy {0}'.format(accuracy(layer_states[2], valid_target)))\n",
    "        print('Epoch {0}: average testing CE {1}'.format(epoch+1, epoch_train_CE))\n",
    "\n",
    "    print('Final model average CE {0}'.format(epoch_train_CE))\n",
    "    print('Accuracy {0}'.format(accuracy(layer_states[2], mini_batch_target)))\n",
    "\n",
    "    layer_states = fprop(valid_input, *all_weights)\n",
    "    validation_CE = loss_function(layer_states[2], valid_target)\n",
    "    print('Final model validation CE {0}'.format(validation_CE))\n",
    "    print('Accuracy {0}'.format(accuracy(layer_states[2], valid_target)))\n",
    "    \n",
    "    layer_states = fprop(test_input, *all_weights)\n",
    "    test_CE = loss_function(layer_states[2], test_target)\n",
    "    print('Final model test CE {0}'.format(test_CE))\n",
    "    print('Accuracy {0}'.format(accuracy(layer_states[2], test_target)))\n",
    "    \n",
    "    print('training took {0:.0f} sec'.format(time.time() - start_time))\n",
    "    \n",
    "    return all_weights\n",
    "a = train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEPCAYAAAC6Kkg/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b3b0b93128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq = [3, 41, 192, 139, 20, 3, 1, 1]\n",
    "x = []\n",
    "for i in range(len(freq)):\n",
    "    x += [(i*200 + 1400)] * freq[i]\n",
    "    \n",
    "# the histogram of the data\n",
    "plt.hist(x, bins=[n*200+1400 for n in range(len(freq))])\n",
    "plt.xlabel('water level')\n",
    "plt.ylabel('frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[['e', 'x', '1'],\n",
       "        ['e', 'x', '2'],\n",
       "        ['e', 'x', '1']],\n",
       "\n",
       "       [['e', 'x', '2'],\n",
       "        ['e', 'x', '1'],\n",
       "        ['e', 'x', '2']],\n",
       "\n",
       "       [['e', 'x', '2'],\n",
       "        ['e', 'x', '2'],\n",
       "        ['e', 'x', '2']]],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([\n",
    "    [0,1,0],\n",
    "    [1,0,1],\n",
    "    [1,1,1]])\n",
    "\n",
    "b = np.array([\n",
    "    ['e', 'x', '1'], \n",
    "    ['e', 'x', '2']])\n",
    "\n",
    "b[a]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
